{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "from conv_util import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "LOG_DIR = './log/'\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "num_steps = 1000\n",
    "batch_size = 256\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "num_classes = 6\n",
    "keep_rate = 1\n",
    "\n",
    "repeat_size = 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('./storks_obs_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0. for i in  range(3)])\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(raw)):\n",
    "    \n",
    "    row = np.array(raw.loc[i][~raw.loc[i].isnull()])\n",
    "    data = row[0:-1].astype(float)\n",
    "    label = row[-1]\n",
    "    \n",
    "    x_axis = data[0::3]\n",
    "    y_axis = data[1::3]\n",
    "    z_axis = data[2::3]\n",
    "    \n",
    "    x_axis = repeat_crop_data (x_axis, repeat_size)\n",
    "    y_axis = repeat_crop_data (y_axis, repeat_size)\n",
    "    z_axis = repeat_crop_data (z_axis, repeat_size)\n",
    "    \n",
    "    mean[0] += x_axis.mean()\n",
    "    mean[1] += y_axis.mean()\n",
    "    mean[2] += z_axis.mean()\n",
    "    \n",
    "    X.append(np.stack((x_axis, y_axis, z_axis)))#.reshape((9, 9, 3)))\n",
    "    Y.append(label)\n",
    "\n",
    "mean = mean/len(raw)\n",
    "    \n",
    "X = np.array(X).reshape((-1, 9, 9, 3))\n",
    "Y = np.array(Y)\n",
    "label_names = np.unique(Y)\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.unique(Y))\n",
    "Y = le.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.1)\n",
    "\n",
    "train_x = train_x.astype(np.float32)\n",
    "test_x = test_x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = tf.Graph()\n",
    "with model_graph.as_default():\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dataset Preparation\n",
    "    trn_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(128).repeat().batch(batch_size)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y)).shuffle(128).repeat().batch(batch_size)\n",
    "    \n",
    "    iterator = tf.data.Iterator.from_structure(trn_ds.output_types, trn_ds.output_shapes)\n",
    "    x, y = iterator.get_next()\n",
    "    \n",
    "    training_init = iterator.make_initializer(trn_ds)\n",
    "    testing_init = iterator.make_initializer(test_ds)\n",
    "    \n",
    "    # Logits and Predictions\n",
    "    logits = build_model(x, keep_prob, input_dim=[9, 9], mean=mean) \n",
    "    prediction = {'classes': tf.argmax(logits, axis=1), \n",
    "                  'prob': tf.nn.softmax(logits, name='prob')}\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    with tf.variable_scope('cross_entropy_loss'):\n",
    "        loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        tf.summary.scalar('cross_entropy_loss', loss_op)\n",
    "    \n",
    "    with tf.variable_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluation\n",
    "    with tf.variable_scope('accuracy'):\n",
    "        correct_pred = tf.equal(prediction['classes'], y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Global Initializer\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    \n",
    "    # Summary\n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    # Global saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'storkes_plain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.9202, Training Accuracy= 0.477\n",
      "Step 50, Minibatch Loss= 0.5225, Training Accuracy= 0.848\n",
      "Step 100, Minibatch Loss= 0.3593, Training Accuracy= 0.906\n",
      "Step 150, Minibatch Loss= 0.3073, Training Accuracy= 0.906\n",
      "Step 200, Minibatch Loss= 0.2676, Training Accuracy= 0.910\n",
      "Step 250, Minibatch Loss= 0.2282, Training Accuracy= 0.934\n",
      "Step 300, Minibatch Loss= 0.2205, Training Accuracy= 0.934\n",
      "Step 350, Minibatch Loss= 0.2237, Training Accuracy= 0.934\n",
      "Step 400, Minibatch Loss= 0.1427, Training Accuracy= 0.969\n",
      "Step 450, Minibatch Loss= 0.1884, Training Accuracy= 0.957\n",
      "Step 500, Minibatch Loss= 0.1614, Training Accuracy= 0.953\n",
      "Step 550, Minibatch Loss= 0.1370, Training Accuracy= 0.961\n",
      "Step 600, Minibatch Loss= 0.1671, Training Accuracy= 0.961\n",
      "Step 650, Minibatch Loss= 0.1184, Training Accuracy= 0.969\n",
      "Step 700, Minibatch Loss= 0.0922, Training Accuracy= 0.977\n",
      "Step 750, Minibatch Loss= 0.1404, Training Accuracy= 0.957\n",
      "Step 800, Minibatch Loss= 0.0682, Training Accuracy= 0.988\n",
      "Step 850, Minibatch Loss= 0.0640, Training Accuracy= 0.988\n",
      "Step 900, Minibatch Loss= 0.1534, Training Accuracy= 0.945\n",
      "Step 950, Minibatch Loss= 0.0843, Training Accuracy= 0.984\n",
      "Step 1000, Minibatch Loss= 0.0874, Training Accuracy= 0.973\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.94140625\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=model_graph) as sess:\n",
    "    \n",
    "    # Run Global Initializer\n",
    "    sess.run(initializer)\n",
    "    \n",
    "    sess.run(training_init)\n",
    "    for step in range(1, num_steps+1):\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={keep_prob: keep_rate})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    sess.run(testing_init)\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={keep_prob: 1.0}))\n",
    "    \n",
    "    # Save model\n",
    "    saver.save(sess, os.path.join(LOG_DIR, 'model_%s'%MODEL_ID, \"final.model.ckpt\"), step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./log/model_storkes_plain/final.model.ckpt-1000\n",
      "Model restored.\n",
      "Testing Accuracy: 0.94140625\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=model_graph) as sess:\n",
    "    \n",
    "    # Restore pretrained model\n",
    "    saver.restore(sess, './log/model_storkes_plain/final.model.ckpt-1000')\n",
    "    print('Model restored.')\n",
    "    \n",
    "    sess.run(testing_init)\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={keep_prob: 1.0}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
